{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the width and height for loading images\n",
    "w, h = 250, 250  # You can adjust these values based on your requirements\n",
    "\n",
    "\n",
    "# List of folders with their respective label1 and label2 values\n",
    "folder_data = [\n",
    "    {'folder': 'C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Comprehensive Disaster Dataset(CDD)\\\\Fire_Disaster\\\\Wild_Fire', 'label1': 'Fire Disaster', 'label2': 'Wild Fire'},\n",
    "    {'folder': 'C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Comprehensive Disaster Dataset(CDD)\\\\Land_Disaster\\\\Drought', 'label1': 'Land Disaster', 'label2': 'Drought'},\n",
    "    {'folder': 'C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Comprehensive Disaster Dataset(CDD)\\\\Non_Damage\\\\human', 'label1': 'Human', 'label2': 'Human'},\n",
    "    {'folder': 'C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Comprehensive Disaster Dataset(CDD)\\\\Non_Damage\\\\Non_Damage_Wildlife_Forest', 'label1': 'Nature', 'label2': 'Undamaged Forest'},\n",
    "    {'folder': \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Sea\", 'label1': 'Sea', 'label2': 'Sea'},\n",
    "    {'folder': \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Comprehensive Disaster Dataset(CDD)\\\\Non_Damage\\\\Rain\", 'label1': 'Rain', 'label2': 'Water Disaster'},\n",
    "    {'folder': 'C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\6th Sem Project\\\\archive\\\\Comprehensive Disaster Dataset(CDD)\\\\Water_Disaster', 'label1': 'Water Disaster', 'label2': 'Water Disaster'}\n",
    "    # Add the remaining folders and label values\n",
    "]\n",
    "\n",
    "# Combine file paths and label values into a list of dictionaries\n",
    "data_list = []\n",
    "\n",
    "for folder_info in folder_data:\n",
    "    folder_path = folder_info['folder']\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    data_list.append({'image_path': image_path, 'label1': folder_info['label1'], 'label2': folder_info['label2']})\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_info['folder']}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Split data into training and testing sets with stratification\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label1'])\n",
    "\n",
    "# Load images and labels into arrays\n",
    "def load_images_and_labels(data):\n",
    "    images = []\n",
    "    labels1 = []\n",
    "    labels2 = []\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            img = load_img(row['image_path'], target_size=(w, h))\n",
    "            img_array = img_to_array(img) / 255.0\n",
    "            images.append(img_array)\n",
    "            labels1.append(row['label1'])\n",
    "            labels2.append(row['label2'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {row['image_path']}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels1), np.array(labels2)\n",
    "\n",
    "# Load images and labels for training and testing sets\n",
    "train_images, train_labels1, train_labels2 = load_images_and_labels(train_data)\n",
    "test_images, test_labels1, test_labels2 = load_images_and_labels(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label1\n",
      "Nature            4900\n",
      "Water Disaster     602\n",
      "Sea                556\n",
      "Fire Disaster      361\n",
      "Human              297\n",
      "Rain               207\n",
      "Land Disaster      191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df['label'] is the column for which you want to get the value counts\n",
    "\n",
    "value_counts = df['label1'].value_counts()\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Determine the number of unique classes for label1 and label2\n",
    "num_classes1 = len(df['label1'].unique())\n",
    "num_classes2 = len(df['label2'].unique())\n",
    "\n",
    "# Create a LabelEncoder for each category\n",
    "label_encoder1 = LabelEncoder()\n",
    "label_encoder2 = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the combined set of training and test labels\n",
    "combined_labels1 = np.concatenate([train_labels1, test_labels1])\n",
    "combined_labels2 = np.concatenate([train_labels2, test_labels2])\n",
    "\n",
    "label_encoder1.fit(combined_labels1)\n",
    "label_encoder2.fit(combined_labels2)\n",
    "\n",
    "# Transform string labels to numerical indices\n",
    "train_labels1_indices = label_encoder1.transform(train_labels1)\n",
    "train_labels2_indices = label_encoder2.transform(train_labels2)\n",
    "\n",
    "test_labels1_indices = label_encoder1.transform(test_labels1)\n",
    "test_labels2_indices = label_encoder2.transform(test_labels2)\n",
    "\n",
    "# Apply to_categorical on numerical indices\n",
    "train_labels1_one_hot = to_categorical(train_labels1_indices, num_classes=num_classes1)\n",
    "train_labels2_one_hot = to_categorical(train_labels2_indices, num_classes=num_classes2)\n",
    "\n",
    "test_labels1_one_hot = to_categorical(test_labels1_indices, num_classes=num_classes1)\n",
    "test_labels2_one_hot = to_categorical(test_labels2_indices, num_classes=num_classes2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (250, 250, 3)  # Adjust based on your image size and channels\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "# Convolutional block\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "flatten = Flatten()(pool2)\n",
    "\n",
    "# First output branch for label1\n",
    "dense1 = Dense(128, activation='relu')(flatten)\n",
    "output1 = Dense(7, activation='softmax', name='output1')(dense1)\n",
    "\n",
    "# Combine both output branches\n",
    "model = Model(inputs=input_layer, outputs=[output1])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# Create model1 with output1 branch\n",
    "model1 = Model(inputs=model.input, outputs=model.get_layer('output1').output)\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "es1 = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=2, verbose=1, restore_best_weights=True)\n",
    "mc1 = ModelCheckpoint(\"Finally_Final_DDS.keras\", monitor=\"val_accuracy\", verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit method for training model1\n",
    "#model1.fit(\n",
    " #   x=train_images,\n",
    "  #  y=train_labels1_one_hot,\n",
    "   # epochs=10,\n",
    "    #validation_data=(test_images, test_labels1_one_hot)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "178/178 [==============================] - ETA: 0s - loss: 0.7997 - accuracy: 0.8183\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88616, saving model to Finally_Final_DDS.keras\n",
      "178/178 [==============================] - 107s 583ms/step - loss: 0.7997 - accuracy: 0.8183 - val_loss: 0.3268 - val_accuracy: 0.8862\n",
      "Epoch 2/3\n",
      "178/178 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9320\n",
      "Epoch 2: val_accuracy improved from 0.88616 to 0.92621, saving model to Finally_Final_DDS.keras\n",
      "178/178 [==============================] - 98s 551ms/step - loss: 0.2041 - accuracy: 0.9320 - val_loss: 0.2347 - val_accuracy: 0.9262\n",
      "Epoch 3/3\n",
      "178/178 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9759\n",
      "Epoch 3: val_accuracy did not improve from 0.92621\n",
      "178/178 [==============================] - 97s 544ms/step - loss: 0.0885 - accuracy: 0.9759 - val_loss: 0.2749 - val_accuracy: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d28cbfead0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit method for training model2\n",
    "model1.fit(\n",
    "    x=train_images,\n",
    "    y=train_labels1_one_hot,\n",
    "    epochs=3,\n",
    "    validation_data=(test_images, test_labels1_one_hot),\n",
    "    callbacks=[es1, mc1]    \n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 4s 90ms/step - loss: 0.2749 - accuracy: 0.9178\n",
      "Model1 Accuracy: 91.78%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model1\n",
    "loss1, accuracy1 = model1.evaluate(test_images, test_labels1_one_hot, verbose=1)\n",
    "print(f\"Model1 Accuracy: {accuracy1 * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
